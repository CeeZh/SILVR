# Reference: https://github.com/yale-nlp/MMVU

import json
import os
import math
import faulthandler
import argparse
import asyncio
import openai
from tqdm import tqdm
from tqdm.asyncio import tqdm_asyncio
from pydantic import BaseModel
import aiolimiter
import random
from pathlib import Path

from utils import save_json, load_json, save_pkl, load_pkl, makedir

from openai import AsyncAzureOpenAI, OpenAIError, AsyncOpenAI
from dotenv import load_dotenv
load_dotenv()


INSTRUCTION="""Evaluate whether the model's final answer is correct by comparing it to the ground-truth answer provided for the given question.

You should first extract the final answer from the model's response, and then compare the extracted answer with the ground-truth answer to determine its accuracy.
"""
MULTI_CHOICE_INSTRUCTION=INSTRUCTION + "Output your response in the following structured format:\n" + """{
    "extracted_answer": // str value "A" "B" "C" "D" "E", should be a single character
    "correct": // boolean value, True if the answer is correct, False otherwise
}
"""

OPEN_ENDED_INSTRUCTION=INSTRUCTION + """The final answer generated by the model does not need to match the ground-truth answer word-for-word. However, it should only be considered correct if it demonstrates the exact same technique or concept explicitly and unambiguously equivalent to the ground-truth answer.
Output your response in the following structured format:
{
    "extracted_answer": // str value, the short final answer extracted from the model's response, do not hallucinate one that is not present in the response
    "correct": // boolean value, True if the answer is correct, False otherwise
}
"""

class EvaluationOutput(BaseModel):
    extracted_answer: str
    correct: bool

async def _throttled_openai_chat_completion_acreate_structure(
    client,
    model: str,
    messages,
    temperature: float,
    max_tokens: int,
    top_p: float,
    limiter: aiolimiter.AsyncLimiter,
    structured_format
):
    async with limiter:
        for _ in range(10):
            try:
                return await client.beta.chat.completions.parse(
                    model=model,
                    messages=messages,
                    temperature=temperature,
                    max_tokens=max_tokens,
                    top_p=top_p,
                    response_format=structured_format,
                )
                
            except openai.RateLimitError as e:
                print("Rate limit exceeded, retrying...")
                await asyncio.sleep(random.randint(10, 20)) 
            except openai.BadRequestError as e:
                print(e)
                return None
            except OpenAIError as e:
                print(e)
                await asyncio.sleep(random.randint(5, 10))
        return None

async def generate_from_openai_chat_completion_structured_format(
    client,
    messages,
    engine_name: str,
    structured_format,
    temperature: float = 1.0,
    max_tokens: int = 512,
    top_p: float = 1.0,
    requests_per_minute: int = 100,
):
    # https://chat.openai.com/share/09154613-5f66-4c74-828b-7bd9384c2168
    delay = 60.0 / requests_per_minute
    limiter = aiolimiter.AsyncLimiter(1, delay)
    async_responses = [
        _throttled_openai_chat_completion_acreate_structure(
            client,
            model=engine_name,
            messages=message,
            temperature=temperature,
            max_tokens=max_tokens,
            top_p=top_p,
            limiter=limiter,
            structured_format=structured_format
        )
        for message in messages
    ]
    
    responses = await tqdm_asyncio.gather(*async_responses)
    
    outputs = []
    for response in responses:
        try:
            response = response.choices[0].message.parsed
        except:
            response = None
        outputs.append(response)
    return outputs


def prepare_evaluation_message(example, response):
    user_prompt = ""
    question_type = example["question_type"]
    if question_type == "multiple-choice":
        # optionized_list = [f"{key}: {value}" for i, (key, value) in enumerate(example['choices'].items())]
        # optionized_str = "\n".join(optionized_list)
        optionized_str = ""
        for i, option in enumerate(example['choices']):
            optionized_str += f"{chr(ord('A')+i)}: {option}\n"
        question_context = f"Question: {example['question']}\n\nOptions:\n{optionized_str}"
    elif question_type == "open-ended":
        question_context = f"Question: {example['question']}"
    
    gt_answer = f"Ground Truth Answer: {example['answer']}"
    model_response = f"Model Response to the Question: {response}"
    
    user_prompt = f"{question_context}\n\n{gt_answer}\n\n{model_response}"
    
    if question_type == "open-ended":
        message = [
            {"role": "system", "content": OPEN_ENDED_INSTRUCTION},
            {"role": "user", "content": user_prompt},
        ]
    else:
        message = [
            {"role": "system", "content": MULTI_CHOICE_INSTRUCTION},
            {"role": "user", "content": user_prompt},
        ]
    return message

async def get_acc_async(examples, client):
    evaluation_messages = [
        prepare_evaluation_message(example, example['response'])
        for example in examples
    ]
    
    os.makedirs("cache", exist_ok=True)
    json.dump(evaluation_messages, open("cache/evaluation_messages.json", "w"), indent=4, ensure_ascii=False)
    
    # Just await directly, no asyncio.run():
    outputs = await generate_from_openai_chat_completion_structured_format(     
        client=client, 
        messages=evaluation_messages,
        engine_name="gpt-4o", 
        max_tokens=128,
        requests_per_minute=350,
        structured_format=EvaluationOutput
    )
    
    count = 0
    results = []
    for example, output in zip(examples, outputs):
        result = {
            "id": example["id"],
            "question": example["question"],
            "choices": example["choices"],
            "response": example["response"],
            "ground_truth_answer": example["answer"],  
        }
        try:
            result["extracted_answer"] = output.extracted_answer
            result["correct"] = output.correct
        except Exception as e:
            result["extracted_answer"] = ""
            result["correct"] = False
            print(f"Error: {e}")
        
        results.append(result)
        count += result["correct"]
            
    return count / len(examples), results


async def main(output_dir, eval_dir):
    examples = []
    output_file_names = []
    
    for output_file in os.listdir(output_dir):
        if not output_file.endswith(".json"):
            continue

        examples_path = os.path.join(output_dir, output_file)
        example = load_json(examples_path)
        examples.append(example)
        output_file_names.append(output_file)

    client = AsyncOpenAI(
        api_key=os.getenv("OPENAI_API_KEY"),
    )
    accuracy, outputs = await get_acc_async(examples, client)
    print(f"Accuracy: {accuracy}")
    results = {
        'accuracy': accuracy,
        'outputs': outputs
    }
    save_json(results, eval_dir)


def eval_mmvu(output_dir, qa_data):
    eval_output_dir = Path(output_dir).parent / 'eval_results.json'
    asyncio.run(main(output_dir, str(eval_output_dir)))
    return {}


if __name__ == "__main__":
    asyncio.run(main())